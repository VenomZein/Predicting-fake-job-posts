{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 976879,
          "sourceType": "datasetVersion",
          "datasetId": 533871
        }
      ],
      "dockerImageVersionId": 29907,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "NLP EDA (Spacy & Pipeline) 98% accuracy!",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'real-or-fake-fake-jobposting-prediction:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F533871%2F976879%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240314%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240314T051444Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D9391ee349dd61897689c378b840a83dddae449d7c40c972c1a214098b61b5f0b4ac836dbe960eaa94013d69e266b9b77a362be151763d8d4193c71b31a43bcfc32890046b577f3105959bef5c8705e4f5ad552fae511e6fdcc026f84087314d829581b7addf5707d793221ede0596db22849c7f3e642bddb276bdeb662757e1ca4cc9a9344ed826751f78d85fd9bfd9a3d43e4e2ff61ec5004e86ce93b2d0ac241fdd8b4daa002f25b5db91aa1f3b284b91809069d709e600a6b8f67c51fbfac1c971e41dafccdf81671c0d57ea2389f5da0b9c4c9f13edf7ef4f42e4f1dbfbc071c448e54bdb6f684483a10b7d42f02f3c4013340001e52b70070bb9d1efa19'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "KR0a26sqYmMJ"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2023-04-08T01:09:41.422486Z",
          "iopub.execute_input": "2023-04-08T01:09:41.422833Z",
          "iopub.status.idle": "2023-04-08T01:09:41.432905Z",
          "shell.execute_reply.started": "2023-04-08T01:09:41.422774Z",
          "shell.execute_reply": "2023-04-08T01:09:41.431854Z"
        },
        "trusted": true,
        "id": "d8LRorMlYmML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import missingno\n",
        "import plotly.express as px\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.base import TransformerMixin\n",
        "from sklearn.metrics import accuracy_score, recall_score, plot_confusion_matrix\n",
        "\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "import spacy\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "from spacy.lang.en import English\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "execution": {
          "iopub.status.busy": "2023-04-08T01:09:41.434472Z",
          "iopub.execute_input": "2023-04-08T01:09:41.434775Z",
          "iopub.status.idle": "2023-04-08T01:09:45.480846Z",
          "shell.execute_reply.started": "2023-04-08T01:09:41.434733Z",
          "shell.execute_reply": "2023-04-08T01:09:45.479823Z"
        },
        "trusted": true,
        "id": "1I2X7Os7YmMM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('/kaggle/input/real-or-fake-fake-jobposting-prediction/fake_job_postings.csv')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-08T01:09:45.482093Z",
          "iopub.execute_input": "2023-04-08T01:09:45.482436Z",
          "iopub.status.idle": "2023-04-08T01:09:46.509429Z",
          "shell.execute_reply.started": "2023-04-08T01:09:45.482385Z",
          "shell.execute_reply": "2023-04-08T01:09:46.508475Z"
        },
        "trusted": true,
        "id": "kRQqUhc-YmMM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-08T01:09:46.510575Z",
          "iopub.execute_input": "2023-04-08T01:09:46.510827Z",
          "iopub.status.idle": "2023-04-08T01:09:46.539144Z",
          "shell.execute_reply.started": "2023-04-08T01:09:46.510787Z",
          "shell.execute_reply": "2023-04-08T01:09:46.53803Z"
        },
        "trusted": true,
        "id": "t6C5ccVfYmMN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checking missing data in our dataframe.\n",
        "missingno.matrix(data)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-08T01:09:46.541411Z",
          "iopub.execute_input": "2023-04-08T01:09:46.54169Z",
          "iopub.status.idle": "2023-04-08T01:09:47.158268Z",
          "shell.execute_reply.started": "2023-04-08T01:09:46.541644Z",
          "shell.execute_reply": "2023-04-08T01:09:47.157557Z"
        },
        "trusted": true,
        "id": "OeEJ6kWJYmMN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* As we can see their are a lot of null values in our dataset, so we need to figure out something later about it."
      ],
      "metadata": {
        "id": "MUGBVT3BYmMN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(data.columns)\n",
        "data.describe()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-08T01:09:47.160053Z",
          "iopub.execute_input": "2023-04-08T01:09:47.16046Z",
          "iopub.status.idle": "2023-04-08T01:09:47.192584Z",
          "shell.execute_reply.started": "2023-04-08T01:09:47.160422Z",
          "shell.execute_reply": "2023-04-08T01:09:47.19199Z"
        },
        "trusted": true,
        "id": "HEXAWkODYmMP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* From describing our data we get to know that their are 4 columns named as job_id, telecommuting, has_company_logo and has_questions features which have numerical data. So we can easily remove these columns as they are of no use in text classification problems.\n",
        "* We can also see one numerical feature 'fraudulent' is basically column on which our model will be trained and predicted."
      ],
      "metadata": {
        "id": "VF8poRfxYmMQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Now lets see how many jobs posted are fraud and real.\n",
        "sns.countplot(data.fraudulent)\n",
        "data.groupby('fraudulent').count()['title'].reset_index().sort_values(by='title',ascending=False)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-08T01:09:47.193631Z",
          "iopub.execute_input": "2023-04-08T01:09:47.194014Z",
          "iopub.status.idle": "2023-04-08T01:09:47.337216Z",
          "shell.execute_reply.started": "2023-04-08T01:09:47.193978Z",
          "shell.execute_reply": "2023-04-08T01:09:47.336086Z"
        },
        "trusted": true,
        "id": "NN9Qx8NqYmMQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* From the plot we can see their are very few fraud jobs posted.\n",
        "* Our data is very much imbalanced so its a hard work to make a good classifier, we will try best :-)"
      ],
      "metadata": {
        "id": "1FdlfH4-YmMQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Now let's fill the nan values and get rid of the columns which are of no use to make things simpler.**"
      ],
      "metadata": {
        "id": "7Y2JEhvsYmMQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "columns=['job_id', 'telecommuting', 'has_company_logo', 'has_questions', 'salary_range', 'employment_type']\n",
        "for col in columns:\n",
        "    del data[col]\n",
        "\n",
        "data.fillna(' ', inplace=True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-08T01:09:47.338754Z",
          "iopub.execute_input": "2023-04-08T01:09:47.339166Z",
          "iopub.status.idle": "2023-04-08T01:09:47.365209Z",
          "shell.execute_reply.started": "2023-04-08T01:09:47.339105Z",
          "shell.execute_reply": "2023-04-08T01:09:47.364097Z"
        },
        "trusted": true,
        "id": "BrhB04O7YmMR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-08T01:09:47.366449Z",
          "iopub.execute_input": "2023-04-08T01:09:47.366697Z",
          "iopub.status.idle": "2023-04-08T01:09:47.387027Z",
          "shell.execute_reply.started": "2023-04-08T01:09:47.366656Z",
          "shell.execute_reply": "2023-04-08T01:09:47.386301Z"
        },
        "trusted": true,
        "id": "26IHAfiiYmMR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Let's check which country posts most number of jobs.**"
      ],
      "metadata": {
        "id": "FnuvfzlLYmMR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split(location):\n",
        "    l = location.split(',')\n",
        "    return l[0]\n",
        "\n",
        "data['country'] = data.location.apply(split)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-08T01:09:47.388721Z",
          "iopub.execute_input": "2023-04-08T01:09:47.389085Z",
          "iopub.status.idle": "2023-04-08T01:09:47.403498Z",
          "shell.execute_reply.started": "2023-04-08T01:09:47.389032Z",
          "shell.execute_reply": "2023-04-08T01:09:47.402755Z"
        },
        "trusted": true,
        "id": "5IVqM35-YmMR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "country = dict(data.country.value_counts()[:11])\n",
        "del country[' ']\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.title('No. of job postings country wise', size=20)\n",
        "plt.bar(country.keys(), country.values())\n",
        "plt.ylabel('No. of jobs', size=10)\n",
        "plt.xlabel('Countries', size=10)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-08T01:09:47.40456Z",
          "iopub.execute_input": "2023-04-08T01:09:47.404933Z",
          "iopub.status.idle": "2023-04-08T01:09:47.557245Z",
          "shell.execute_reply.started": "2023-04-08T01:09:47.404886Z",
          "shell.execute_reply": "2023-04-08T01:09:47.556598Z"
        },
        "trusted": true,
        "id": "dAEqlLPHYmMS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Most number of jobs are posted by US."
      ],
      "metadata": {
        "id": "vA8jdb_zYmMS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's check about which type of experience is required in most number of jobs."
      ],
      "metadata": {
        "id": "AgrCTh4hYmMS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "experience = dict(data.required_experience.value_counts())\n",
        "del experience[' ']\n",
        "plt.bar(experience.keys(), experience.values())\n",
        "plt.xlabel('Experience', size=10)\n",
        "plt.ylabel('no. of jobs', size=10)\n",
        "plt.xticks(rotation=35)\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-08T01:09:47.558202Z",
          "iopub.execute_input": "2023-04-08T01:09:47.558632Z",
          "iopub.status.idle": "2023-04-08T01:09:47.689637Z",
          "shell.execute_reply.started": "2023-04-08T01:09:47.558589Z",
          "shell.execute_reply": "2023-04-08T01:09:47.688998Z"
        },
        "trusted": true,
        "id": "mu-XyJIZYmMS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# title of jobs which are frequent.\n",
        "print(data.title.value_counts()[:10])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-08T01:09:47.690771Z",
          "iopub.execute_input": "2023-04-08T01:09:47.691123Z",
          "iopub.status.idle": "2023-04-08T01:09:47.704658Z",
          "shell.execute_reply.started": "2023-04-08T01:09:47.691067Z",
          "shell.execute_reply": "2023-04-08T01:09:47.703457Z"
        },
        "trusted": true,
        "id": "-CvCEhC9YmMS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Now we should combine our text in a single column to start cleaning our data.**"
      ],
      "metadata": {
        "id": "6Ql4XzYnYmMT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data['text']=data['title']+' '+data['location']+' '+data['company_profile']+' '+data['description']+' '+data['requirements']+' '+data['benefits']\n",
        "del data['title']\n",
        "del data['location']\n",
        "del data['department']\n",
        "del data['company_profile']\n",
        "del data['description']\n",
        "del data['requirements']\n",
        "del data['benefits']\n",
        "del data['required_experience']\n",
        "del data['required_education']\n",
        "del data['industry']\n",
        "del data['function']\n",
        "del data['country']"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-08T01:09:47.70579Z",
          "iopub.execute_input": "2023-04-08T01:09:47.706156Z",
          "iopub.status.idle": "2023-04-08T01:09:47.859019Z",
          "shell.execute_reply.started": "2023-04-08T01:09:47.706101Z",
          "shell.execute_reply": "2023-04-08T01:09:47.858346Z"
        },
        "trusted": true,
        "id": "Xeuca2lqYmMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-08T01:09:47.859988Z",
          "iopub.execute_input": "2023-04-08T01:09:47.860254Z",
          "iopub.status.idle": "2023-04-08T01:09:47.869478Z",
          "shell.execute_reply.started": "2023-04-08T01:09:47.860189Z",
          "shell.execute_reply": "2023-04-08T01:09:47.867998Z"
        },
        "trusted": true,
        "id": "jFe6aLl9YmMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Now lets see what type of words are frequent in fraud and actual jobs using wordclouds**"
      ],
      "metadata": {
        "id": "lNlGgV3pYmMU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fraudjobs_text = data[data.fraudulent==1].text\n",
        "actualjobs_text = data[data.fraudulent==0].text"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-08T01:09:47.870363Z",
          "iopub.execute_input": "2023-04-08T01:09:47.870599Z",
          "iopub.status.idle": "2023-04-08T01:09:47.883425Z",
          "shell.execute_reply.started": "2023-04-08T01:09:47.87056Z",
          "shell.execute_reply": "2023-04-08T01:09:47.882281Z"
        },
        "trusted": true,
        "id": "b7B93sxIYmMU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "STOPWORDS = spacy.lang.en.stop_words.STOP_WORDS\n",
        "plt.figure(figsize = (16,14))\n",
        "wc = WordCloud(min_font_size = 3,  max_words = 3000 , width = 1600 , height = 800 , stopwords = STOPWORDS).generate(str(\" \".join(fraudjobs_text)))\n",
        "plt.imshow(wc,interpolation = 'bilinear')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-08T01:09:47.88463Z",
          "iopub.execute_input": "2023-04-08T01:09:47.884916Z",
          "iopub.status.idle": "2023-04-08T01:09:55.709649Z",
          "shell.execute_reply.started": "2023-04-08T01:09:47.884871Z",
          "shell.execute_reply": "2023-04-08T01:09:55.708542Z"
        },
        "trusted": true,
        "id": "NWtsAz5FYmMV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (16,14))\n",
        "wc = WordCloud(min_font_size = 3,  max_words = 3000 , width = 1600 , height = 800 , stopwords = STOPWORDS).generate(str(\" \".join(actualjobs_text)))\n",
        "plt.imshow(wc,interpolation = 'bilinear')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-08T01:09:55.710598Z",
          "iopub.execute_input": "2023-04-08T01:09:55.710822Z",
          "iopub.status.idle": "2023-04-08T01:10:23.29479Z",
          "shell.execute_reply.started": "2023-04-08T01:09:55.710783Z",
          "shell.execute_reply": "2023-04-08T01:10:23.293354Z"
        },
        "trusted": true,
        "id": "OAtyZ28rYmMV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cleaning Data"
      ],
      "metadata": {
        "id": "IRHK7jBXYmMV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Creating a function that accepts a sentence as input and processes the sentence into tokens, performing lemmatization, lowercasing, and removing stop words.\n",
        "* The function that i have used to do these work is found here https://www.dataquest.io/blog/tutorial-text-classification-in-python-using-spacy/, i know that i cant write so neat so i just taken those functions."
      ],
      "metadata": {
        "id": "3uYfGEsDYmMV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create our list of punctuation marks\n",
        "punctuations = string.punctuation\n",
        "\n",
        "# Create our list of stopwords\n",
        "nlp = spacy.load('en')\n",
        "stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
        "\n",
        "# Load English tokenizer, tagger, parser, NER and word vectors\n",
        "parser = English()\n",
        "\n",
        "# Creating our tokenizer function\n",
        "def spacy_tokenizer(sentence):\n",
        "    # Creating our token object, which is used to create documents with linguistic annotations.\n",
        "    mytokens = parser(sentence)\n",
        "\n",
        "    # Lemmatizing each token and converting each token into lowercase\n",
        "    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n",
        "\n",
        "    # Removing stop words\n",
        "    mytokens = [ word for word in mytokens if word not in stop_words and word not in punctuations ]\n",
        "\n",
        "    # return preprocessed list of tokens\n",
        "    return mytokens"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-08T01:10:23.296295Z",
          "iopub.execute_input": "2023-04-08T01:10:23.296686Z",
          "iopub.status.idle": "2023-04-08T01:10:24.182992Z",
          "shell.execute_reply.started": "2023-04-08T01:10:23.296619Z",
          "shell.execute_reply": "2023-04-08T01:10:24.182026Z"
        },
        "trusted": true,
        "id": "iPTNhTzqYmMV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom transformer using spaCy\n",
        "class predictors(TransformerMixin):\n",
        "    def transform(self, X, **transform_params):\n",
        "        # Cleaning Text\n",
        "        return [clean_text(text) for text in X]\n",
        "\n",
        "    def fit(self, X, y=None, **fit_params):\n",
        "        return self\n",
        "\n",
        "    def get_params(self, deep=True):\n",
        "        return {}\n",
        "\n",
        "# Basic function to clean the text\n",
        "def clean_text(text):\n",
        "    # Removing spaces and converting text into lowercase\n",
        "    return text.strip().lower()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-08T01:10:24.184107Z",
          "iopub.execute_input": "2023-04-08T01:10:24.18438Z",
          "iopub.status.idle": "2023-04-08T01:10:24.191373Z",
          "shell.execute_reply.started": "2023-04-08T01:10:24.184341Z",
          "shell.execute_reply": "2023-04-08T01:10:24.190288Z"
        },
        "trusted": true,
        "id": "ESwXM2ibYmMV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating our bag of words\n",
        "bow_vector = CountVectorizer(tokenizer = spacy_tokenizer, ngram_range=(1,3))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-08T01:10:24.192875Z",
          "iopub.execute_input": "2023-04-08T01:10:24.193312Z",
          "iopub.status.idle": "2023-04-08T01:10:24.201812Z",
          "shell.execute_reply.started": "2023-04-08T01:10:24.193252Z",
          "shell.execute_reply": "2023-04-08T01:10:24.200783Z"
        },
        "trusted": true,
        "id": "DHSzFjt2YmMW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* BoW converts text into the matrix of occurrence of words within a given document. It focuses on whether given words occurred or not in the document, and it generates a matrix that we might see referred to as a BoW matrix or a document term matrix."
      ],
      "metadata": {
        "id": "jHlETIakYmMW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# splitting our data in train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.text, data.fraudulent, test_size=0.3)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-08T01:10:24.202907Z",
          "iopub.execute_input": "2023-04-08T01:10:24.203169Z",
          "iopub.status.idle": "2023-04-08T01:10:24.2168Z",
          "shell.execute_reply.started": "2023-04-08T01:10:24.203126Z",
          "shell.execute_reply": "2023-04-08T01:10:24.214954Z"
        },
        "trusted": true,
        "id": "VAEaQiPpYmMW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating Model"
      ],
      "metadata": {
        "id": "P5KYjxZ6YmMW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* We are creating a pipeline with three components: a cleaner, a vectorizer, and a classifier. The cleaner uses our predictors class object to clean and preprocess the text. The vectorizer uses countvector objects to create the bag of words matrix for our text. The classifier is an object that performs the logistic regression to classify the sentiments."
      ],
      "metadata": {
        "id": "zDSvBxeaYmMW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def models(x):\n",
        "\n",
        "\n",
        "  if x == LogisticRegression:\n",
        "    clf = LogisticRegression()\n",
        "\n",
        "  elif x == RandomForest:\n",
        "    clf = RandomForestClassifier()\n",
        "\n",
        "  elif x == SVC:\n",
        "    clf = SVC()\n",
        "\n",
        "  elif x == XGBoost:\n",
        "    clf = XGBClassifier()\n",
        "\n",
        "  else:\n",
        "    print(\"wrong input\")\n",
        "    exit()\n",
        "\n",
        "# Create pipeline using Bag of Words\n",
        "  pipe = Pipeline([(\"cleaner\", predictors()),\n",
        "                  ('vectorizer', bow_vector),\n",
        "                  ('classifier', clf)])\n",
        "\n",
        "  # Fitting model\n",
        "  pipe.fit(X_train, y_train)\n",
        "\n",
        "  # Predicting with a test dataset\n",
        "  predicted = pipe.predict(X_test)\n",
        "\n",
        "  # Model Accuracy\n",
        "  print(x , \"Accuracy: \" , accuracy_score(y_test, predicted))\n",
        "  print(x , \"Recall: \", recall_score(y_test, predicted))\n",
        "\n",
        "  #plot confusion matrix\n",
        "  plot_confusion_matrix(pipe, X_test, y_test, cmap='Blues', values_format=' ')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-08T01:10:24.218176Z",
          "iopub.execute_input": "2023-04-08T01:10:24.21852Z",
          "iopub.status.idle": "2023-04-08T01:10:24.227176Z",
          "shell.execute_reply.started": "2023-04-08T01:10:24.218469Z",
          "shell.execute_reply": "2023-04-08T01:10:24.226314Z"
        },
        "trusted": true,
        "id": "oaPUokI7YmMW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models(LogisticRegression)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-08T01:10:24.22831Z",
          "iopub.execute_input": "2023-04-08T01:10:24.22859Z",
          "iopub.status.idle": "2023-04-08T01:12:38.076536Z",
          "shell.execute_reply.started": "2023-04-08T01:10:24.228547Z",
          "shell.execute_reply": "2023-04-08T01:12:38.075234Z"
        },
        "trusted": true,
        "id": "m_WcR-ezYmMX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}